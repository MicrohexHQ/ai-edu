<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 19.3 通用的循环神经网络

### 19.3.1 提出问题

在19.1节和19.2节的情况不同，都是预知时间步长度，然后以纯手工方式搭建循环神经网络。

||回波检测问题|二进制减法问题|
|---|---|---|
|时间步|2|4|
|网络输出类别|回归|二分类|
|网络输出形式|最后一个时间步输出|每一个时间步都有输出|
|批大小|1|1|
|有无偏移值|有|无|

如果后面再遇到多分类情况，或者其它参数有变化的话，我们不能像19.1节和19.2节那样纯手写代码，而是要抽象出来，写一个比较通用的框架。

“比较通用”是什么意思呢？那就是应该满足以下条件：

1. 既可以支持分类网络（二分类和多分类），也可以支持回归网络
2. 每一个时间步可以有输出并且有监督学习信号，也可以只在最后一个时间步有输出
3. 第一个时间步的前向计算中不包含前一个时间步的隐层状态值（因为前面没有时间步）
4. 最后一个时间步的反向传播中不包含下一个时间步的回传误差（因为后面没有时间步）
5. 可以指定超参数进行网络训练，如：学习率、批大小、最大训练次数、输入层尺寸、隐层神经元数量、输出层尺寸等等
6. 可以保存训练结果并可以在以后加载参数，避免重新训练

### 19.3.2 时间步类的设计

时间步类（timestep）的设计是核心，它体现了循环神经网络的核心概念。下面的代码是该类的初始化函数：

```Python
class timestep(object):
    def __init__(self, net_type, output_type, isFirst=False, isLast=False):
        self.isFirst = isFirst
        self.isLast = isLast
        self.netType = net_type
        if (output_type == OutputType.EachStep):
            self.needOutput = True
        elif (output_type == OutputType.LastStep and isLast):
            self.needOutput = True
        else:
            self.needOutput = False
```

- isFirst和isLast参数指定了该实例是否为第一个时间步或最后一个时间步
- netType参数指定了网络类型，回归、二分类、多分类，三种选择
- output_type结合isLast，可以指定该时间步是否有输出，如果是最后一个时间步，肯定有输出；如果不是最后一个时间步，并且如果output_type是OutputType.EachStep，则每个时间步都有输出，否则就没有输出。最后的判断结果记录在self.needOutput上

#### 前向计算

```Python
    def forward(self, x, U, bu, V, bv, W, prev_s):
        self.U = U
        self.bu = bu
        self.V = V
        self.bv = bv
        self.W = W
        self.x = x

        if (self.isFirst):
            self.h = np.dot(x, U) + self.bu
        else:
            self.h = np.dot(x, U) + np.dot(prev_s, W) + self.bu
        #endif

        self.s = Tanh().forward(self.h)

        if (self.needOutput):
            self.z = np.dot(self.s, V) + self.bv
            if (self.netType == NetType.BinaryClassifier):
                self.a = Logistic().forward(self.z)
            elif (self.netType == NetType.MultipleClassifier):
                self.a = Softmax().forward(self.z)
            else:
                self.a = self.z
            #endif
        #endif
```

- 如果是第一个时间步，则prev_s参数为None，不需要计算在内，否则就需要。
- 如果是二分类，最后的输出用Logistic()函数；如果是多分类，用Softmax()函数；如果是回归，直接self.a = self.z

#### 反向传播

```Python
    def backward(self, y, prev_s, next_dh):
        if (self.isLast):
            assert(self.needOutput == True)
            self.dz = self.a - y
            self.dh = np.dot(self.dz, self.V.T) * Tanh().backward(self.s)
            self.dV = np.dot(self.s.T, self.dz)
        else:
            assert(next_dh is not None)
            if (self.needOutput):
                self.dz = self.a - y
                self.dh = (np.dot(self.dz, self.V.T) + np.dot(next_dh, self.W.T)) * Tanh().backward(self.s)
                self.dV = np.dot(self.s.T, self.dz)
            else:
                self.dz = np.zeros_like(y)
                self.dh = np.dot(next_dh, self.W.T) * Tanh().backward(self.s)
                self.dV = np.zeros_like(self.V)
            #endif
        #endif
        self.dbv = np.sum(self.dz, axis=0, keepdims=True)
        self.dbu = np.sum(self.dh, axis=0, keepdims=True)

        self.dU = np.dot(self.x.T, self.dh)

        if (self.isFirst):
            self.dW = np.zeros_like(self.W)
        else:
            self.dW = np.dot(prev_s.T, self.dh)
        # end if
```
- 如果是最后一个时间步，则肯定要有监督学习信号，因此会计算dz、dh、dV等参数
- 如果不是最后一个时间步，并且有输出，则需要计算dz、dh、dV等参数，并且在计算dh时，需要考虑后一个时间步的next_dh传入
- 如果不是最后一个时间步，并且没有有输出，则只计算从后面的时间步传回来的next_dh
- 如果是第一个时间步，则dW为0，因为prev_s为None（没有前一个时间步传入的状态值），否则需要计算dW


此次的问题并没有指定时间步的长度，而是由网络搭建者自己指定。







### 代码位置

ch19, Level3_Base